<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Ciel&#39;s blog website &lt;3</title>
        <link>https://wwwCielwww.github.io/</link>
        <description>Recent content on Ciel&#39;s blog website &lt;3</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-us</language>
        <lastBuildDate>Mon, 21 Nov 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://wwwCielwww.github.io/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>MA4270 Data Modelling and Computation Notes</title>
        <link>https://wwwCielwww.github.io/ma4270/</link>
        <pubDate>Mon, 21 Nov 2022 00:00:00 +0000</pubDate>
        
        <guid>https://wwwCielwww.github.io/ma4270/</guid>
        <description>&lt;h2 id=&#34;ch-7-boosting&#34;&gt;Ch 7. Boosting&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Decision stumps, where $\theta={s,k,\theta_0}$. $k=$ index of feature, $s=$ sign and $\theta_0$ = threshold.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
h(\boldsymbol{x};\boldsymbol{\theta})=\text{sign}(s(x_k-\theta_0))
$$&lt;/p&gt;
&lt;p&gt;Weighted decision function&lt;/p&gt;
&lt;p&gt;$$
f_M(\boldsymbol{x})=\sum_{m=1}^M\alpha_mh(\boldsymbol{x};\boldsymbol{\theta}_m)
$$&lt;/p&gt;
&lt;p&gt;Individual $h$ are called weak/base learners. AdaBoost helps find good ${\boldsymbol{\theta}_m,\alpha_m}$&lt;/p&gt;
&lt;h3 id=&#34;adaboost&#34;&gt;AdaBoost&lt;/h3&gt;
&lt;p&gt;$\boldsymbol{\hat{\theta}}&lt;em&gt;m=\arg\min&lt;/em&gt;\theta\sum_{t:y_t\neq h(\boldsymbol{x};\boldsymbol{\theta})}w_{m-1}(t)$&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;initialize weights $w_0(t)=\frac{1}{n}$ for $t=1,\dots,n$ ($n=$ size of data set)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;for $m=1,\dots,M$ do&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;$\boldsymbol{\hat{\theta}}_m=\arg\min_\theta\sum_{t:y_t\neq h(\boldsymbol{x};\boldsymbol{\theta})}w_{m-1}(t)$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$\hat{\alpha}_m=\frac{1}{2}\log\frac{1-\hat{\epsilon}_m}{\hat{\epsilon}_m}$, where $\hat{\epsilon}_m$ is the minimal value attained in 2.1&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$w_m(t)=\frac{1}{Z_m}w_{m-1}(t)e^{-y_th(\boldsymbol{x}_t;\hat{\boldsymbol{\theta}}_m)\hat{\alpha}_m}$, where $Z_m$ is the sum of all unnormalized $w_{m-1}(t)$&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;output: $f_M(\boldsymbol{x})=\sum_{m=1}^M\hat{\alpha}_mh(\boldsymbol{x};\hat{\boldsymbol{\theta}}_m) \to\hat{y}=\text{sign}(f_M(\boldsymbol{x}))$&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Note: one can observe that the test error still decreases when training error reaches 0. This is because AdaBoost is implicitly minimizing the margin, hence making the classifier more robust.&lt;/p&gt;
&lt;h2 id=&#34;ch-8-concentration&#34;&gt;Ch 8. Concentration&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Hoeffding&amp;rsquo;s inequality&lt;/strong&gt; Let $Z=X_1+\cdots+X_n$, where the $X_i$ are independent and supported on $[a_i,b_i]$. Then&lt;/p&gt;
&lt;p&gt;$$
\mathbb{P}[\frac{1}{n}\vert Z-\mathbb{E}[Z]\vert &amp;gt;\epsilon]\leq2\exp(-\frac{2n\epsilon^2}{\frac{1}{n}\sum_{i=1}^n(b_i-a_i)^2})
$$&lt;/p&gt;
&lt;h2 id=&#34;ch-9-theory&#34;&gt;Ch 9. Theory&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;$\mathcal{D}={(\boldsymbol{x}&lt;em&gt;i,y_i)}^n&lt;/em&gt;{i=1},(\boldsymbol{x}&lt;em&gt;i,y_i)\sim P&lt;/em&gt;{XY}$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;possible classifiers $f(\boldsymbol{x})$ make up the function class $\mathcal{F}$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;risk/test error $R(f)=\mathbb{E}[l(y,f(\boldsymbol{x})]$ gives the Bayes-optimal classifier&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$f_{erm}=\arg\min_{f\in\mathcal{F}}R_n(f)$ training error $R_n(f)=\frac{1}{n}\sum_{i=1}^nl(y_i,f(\boldsymbol{x}_i))$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;test error = training error + generalization error&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;pac-learning&#34;&gt;PAC Learning&lt;/h3&gt;
&lt;p&gt;To ensure small generalization error, PAC (probably approximately correct) learning seeks to attain a risk within a small value of that chieved by the best $f$ in $\mathcal{F}$.&lt;/p&gt;
&lt;p&gt;Given $l$, $\mathcal{F}$ is PAC-learnable if there exists an algorithm $\mathcal{A}(\mathcal{D}&lt;em&gt;n)$ and a function $\bar{n}(\epsilon,\delta)$ such that: for any $P&lt;/em&gt;{XY}$ and any $\epsilon,\delta\in(0,1)$, if $n\geq\bar{n}(\epsilon,\delta)$, then the following holds with probability at least $1-\delta$:&lt;/p&gt;
&lt;p&gt;$$
R(\hat{f})\leq\min_{f\in\mathcal{F}}R(f)+\epsilon
$$&lt;/p&gt;
&lt;p&gt;$1-\delta\to$ probably correct, $\epsilon\to$ approximately correct, $\bar{n}\to$ sample complexity.&lt;/p&gt;
&lt;h3 id=&#34;finite-function-class&#34;&gt;Finite Function Class&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt; for any bounded loss function in $[0,1]$, any finite function class $\mathcal{F}$ is PAC-learnable with sample complexity $\bar{n}(\epsilon,\delta)=\frac{2}{\epsilon^2}\log\frac{2|\mathcal{F}|}{\delta}$.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Proof&lt;/em&gt;: taking the algorithm to be ERM (empirical risk minimization), apply Hoeffding&amp;rsquo;s inequality&lt;/p&gt;
&lt;p&gt;$$
P[|R(f)-R_n(f)|\geq\epsilon_0]\leq2e^{-2n\epsilon_0^2}.
$$&lt;/p&gt;
&lt;p&gt;We cannot simply substitute $f=f_{erm}$ because the later is not fixed, but rather a random variable depending on $\mathcal{D}$. For finite $\mathcal{F}$,&lt;/p&gt;
&lt;p&gt;$$
\mathcal{P}[\bigcup_{f\in\mathcal{F}}{|R(f)-R_n(f)|&amp;gt;\epsilon_0}]\leq2|\mathcal{F}|e^{-2n\epsilon_0^2}
$$&lt;/p&gt;
&lt;p&gt;By setting the RHS to a target $\delta$, we find the sufficient $n$ as $\frac{1}{2\epsilon_0^2}\log\frac{2|\mathcal{F}|}{\delta}$. Assume the probability $1-\delta$ event occurs, namely $|R(f)-R_n(f)|\leq\epsilon_0$ for all $f\in\mathcal{F}$. Letting $f^*$ be the function that minimizes $R(f)$, we have&lt;/p&gt;
&lt;p&gt;$$
\begin{align*}
R(f_{erm})-R(f^&lt;em&gt;)&amp;amp;=(R(f_{erm})-R_n(f_{erm}))+(R_n(f_{erm})-R_n(f^&lt;/em&gt;))+(R_n(f^&lt;em&gt;)-R(f^&lt;/em&gt;)) \
&amp;amp;\leq\epsilon_0+0+\epsilon_0=2\epsilon_0
\end{align*}
$$&lt;/p&gt;
&lt;p&gt;Setting $\epsilon_0=\frac{\epsilon}{2}$ gives the desired bound, and yields $n=\frac{2}{\epsilon^2}\log\frac{2|\mathcal{F}|}{\delta}$ .&lt;/p&gt;
&lt;h3 id=&#34;infinite-case--vc-dimension&#34;&gt;Infinite Case &amp;amp; VC Dimension&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Intuition&lt;/strong&gt; even with infinitely many hypotheses, there may be only finitely many effective hypotheses.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Definitions&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Growth function/shattering number $S_n(\mathcal{F})=\sup_{\boldsymbol{x}_1,\dots,\boldsymbol{x}_n}|{(f(\boldsymbol{x}_1),\dots,f(\boldsymbol{x}_n)):f\in\mathcal{F}}|$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;This is an integer between $1$ and $2^n$.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;VC dimension $d_{VC}=d_{VC}(\mathcal{F})$ is the largest $k$ such that $S_k(\mathcal{F})=2^k$. If $S_k(\mathcal{F})=2^k$ for all $k$, then we define $d_{VC}=\infty$.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A set of such $k$ points is said to be shattered by $\mathcal{F}$.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;Sauer&amp;rsquo;s lemma&lt;/em&gt;: $S_n(\mathcal{F})\leq\sum_{i=1}^{d_{VC}}\begin{pmatrix}n\i\end{pmatrix}$. For $n\leq d_{VC},S_n(\mathcal{F})=2^n$. Otherwise, $S_n(\mathcal{F})\leq(\frac{d_{VC}}{n})^{d_{VC}}$. A slightly weaker bound is $S_n(\mathcal{F})\leq(n+1)^{d_{VC}}$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt; if $d_{VC}&amp;lt;\infty$, then $\mathcal{F}$ is PAC-learnable under the 0-1 loss with sample complexity&lt;/p&gt;
&lt;p&gt;$$
\bar{n}(\epsilon,\delta)=C\cdot\frac{d_{VC}+\log\frac{1}{\epsilon}}{\epsilon^2}
$$&lt;/p&gt;
&lt;p&gt;for some constant $C$. Conversely, if $d_{VC}=\infty$, then $\mathcal{F}$ is not PAC-leranable.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Hence, $d_{VC}$ serves as a fundamental measure of richness of the function class – to get good generalization, it suffices to have $n\gg d_{VC}$ samples.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Even if $d_{VC}$ is infinite, efficient learning might be possible for a given data distribution $P_{XY}$. The VC theory only establishes the difficulty of worst-case distributions.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Examples&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;rectangular classifier: $d_{VC}=4$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;linear classifier w/ and w/o an offset in dimnension $d$: $d_{VC}=d$ or $d+1$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>MA4270 Data Modelling and Computation Notes</title>
        <link>https://wwwCielwww.github.io/p/ma4270/</link>
        <pubDate>Mon, 21 Nov 2022 00:00:00 +0000</pubDate>
        
        <guid>https://wwwCielwww.github.io/p/ma4270/</guid>
        <description>&lt;h2 id=&#34;ch-7-boosting&#34;&gt;Ch 7. Boosting&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Decision stumps, where $\theta={s,k,\theta_0}$. $k=$ index of feature, $s=$ sign and $\theta_0$ = threshold.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
h(\boldsymbol{x};\boldsymbol{\theta})=\text{sign}(s(x_k-\theta_0))
$$&lt;/p&gt;
&lt;p&gt;Weighted decision function&lt;/p&gt;
&lt;p&gt;$$
f_M(\boldsymbol{x})=\sum_{m=1}^M\alpha_mh(\boldsymbol{x};\boldsymbol{\theta}_m)
$$&lt;/p&gt;
&lt;p&gt;Individual $h$ are called weak/base learners. AdaBoost helps find good ${\boldsymbol{\theta}_m,\alpha_m}$&lt;/p&gt;
&lt;h3 id=&#34;adaboost&#34;&gt;AdaBoost&lt;/h3&gt;
&lt;p&gt;$\boldsymbol{\hat{\theta}}_m=\arg\min_\theta\sum_{t:y_t\neq h(\boldsymbol{x};\boldsymbol{\theta})}w_{m-1}(t)$&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;initialize weights $w_0(t)=\frac{1}{n}$ for $t=1,\dots,n$ ($n=$ size of data set)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;for $m=1,\dots,M$ do&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;$\boldsymbol{\hat{\theta}}_m=\arg\min_\theta\sum_{t:y_t\neq h(\boldsymbol{x};\boldsymbol{\theta})}w_{m-1}(t)$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$\hat{\alpha}_m=\frac{1}{2}\log\frac{1-\hat{\epsilon}_m}{\hat{\epsilon}_m}$, where $\hat{\epsilon}_m$ is the minimal value attained in 2.1&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$w_m(t)=\frac{1}{Z_m}w_{m-1}(t)e^{-y_th(\boldsymbol{x}_t;\hat{\boldsymbol{\theta}}_m)\hat{\alpha}_m}$, where $Z_m$ is the sum of all unnormalized $w_{m-1}(t)$&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;output: $f_M(\boldsymbol{x})=\sum_{m=1}^M\hat{\alpha}_mh(\boldsymbol{x};\hat{\boldsymbol{\theta}}_m) \to\hat{y}=\text{sign}(f_M(\boldsymbol{x}))$&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Note: one can observe that the test error still decreases when training error reaches 0. This is because AdaBoost is implicitly minimizing the margin, hence making the classifier more robust.&lt;/p&gt;
&lt;h2 id=&#34;ch-8-concentration&#34;&gt;Ch 8. Concentration&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Hoeffding&amp;rsquo;s inequality&lt;/strong&gt; Let $Z=X_1+\cdots+X_n$, where the $X_i$ are independent and supported on $[a_i,b_i]$. Then&lt;/p&gt;
&lt;p&gt;$$
\mathbb{P}[\frac{1}{n}\vert Z-\mathbb{E}[Z]\vert &amp;gt;\epsilon]\leq2\exp(-\frac{2n\epsilon^2}{\frac{1}{n}\sum_{i=1}^n(b_i-a_i)^2})
$$&lt;/p&gt;
&lt;h2 id=&#34;ch-9-theory&#34;&gt;Ch 9. Theory&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;$\mathcal{D}={(\boldsymbol{x}_i,y_i)}^n_{i=1},(\boldsymbol{x}_i,y_i)\sim P_{XY}$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;possible classifiers $f(\boldsymbol{x})$ make up the function class $\mathcal{F}$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;risk/test error $R(f)=\mathbb{E}[l(y,f(\boldsymbol{x})]$ gives the Bayes-optimal classifier&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$f_{erm}=\arg\min_{f\in\mathcal{F}}R_n(f)$ training error $R_n(f)=\frac{1}{n}\sum_{i=1}^nl(y_i,f(\boldsymbol{x}_i))$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;test error = training error + generalization error&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;pac-learning&#34;&gt;PAC Learning&lt;/h3&gt;
&lt;p&gt;To ensure small generalization error, PAC (probably approximately correct) learning seeks to attain a risk within a small value of that chieved by the best $f$ in $\mathcal{F}$.&lt;/p&gt;
&lt;p&gt;Given $l$, $\mathcal{F}$ is PAC-learnable if there exists an algorithm $\mathcal{A}(\mathcal{D}_n)$ and a function $\bar{n}(\epsilon,\delta)$ such that: for any $P_{XY}$ and any $\epsilon,\delta\in(0,1)$, if $n\geq\bar{n}(\epsilon,\delta)$, then the following holds with probability at least $1-\delta$:&lt;/p&gt;
&lt;p&gt;$$
R(\hat{f})\leq\min_{f\in\mathcal{F}}R(f)+\epsilon
$$&lt;/p&gt;
&lt;p&gt;$1-\delta\to$ probably correct, $\epsilon\to$ approximately correct, $\bar{n}\to$ sample complexity.&lt;/p&gt;
&lt;h3 id=&#34;finite-function-class&#34;&gt;Finite Function Class&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt; for any bounded loss function in $[0,1]$, any finite function class $\mathcal{F}$ is PAC-learnable with sample complexity $\bar{n}(\epsilon,\delta)=\frac{2}{\epsilon^2}\log\frac{2|\mathcal{F}|}{\delta}$.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Proof&lt;/em&gt;: taking the algorithm to be ERM (empirical risk minimization), apply Hoeffding&amp;rsquo;s inequality&lt;/p&gt;
&lt;p&gt;$$
P[|R(f)-R_n(f)|\geq\epsilon_0]\leq2e^{-2n\epsilon_0^2}.
$$&lt;/p&gt;
&lt;p&gt;We cannot simply substitute $f=f_{erm}$ because the later is not fixed, but rather a random variable depending on $\mathcal{D}$. For finite $\mathcal{F}$,&lt;/p&gt;
&lt;p&gt;$$
\mathcal{P}[\bigcup_{f\in\mathcal{F}}{|R(f)-R_n(f)|&amp;gt;\epsilon_0}]\leq2|\mathcal{F}|e^{-2n\epsilon_0^2}
$$&lt;/p&gt;
&lt;p&gt;By setting the RHS to a target $\delta$, we find the sufficient $n$ as $\frac{1}{2\epsilon_0^2}\log\frac{2|\mathcal{F}|}{\delta}$. Assume the probability $1-\delta$ event occurs, namely $|R(f)-R_n(f)|\leq\epsilon_0$ for all $f\in\mathcal{F}$. Letting $f^*$ be the function that minimizes $R(f)$, we have&lt;/p&gt;
&lt;p&gt;$$
\begin{align*}
R(f_{erm})-R(f^&lt;em&gt;)&amp;amp;=(R(f_{erm})-R_n(f_{erm}))+(R_n(f_{erm})-R_n(f^&lt;/em&gt;))+(R_n(f^&lt;em&gt;)-R(f^&lt;/em&gt;)) \
&amp;amp;\leq\epsilon_0+0+\epsilon_0=2\epsilon_0
\end{align*}
$$&lt;/p&gt;
&lt;p&gt;Setting $\epsilon_0=\frac{\epsilon}{2}$ gives the desired bound, and yields $n=\frac{2}{\epsilon^2}\log\frac{2|\mathcal{F}|}{\delta}$ .&lt;/p&gt;
&lt;h3 id=&#34;infinite-case--vc-dimension&#34;&gt;Infinite Case &amp;amp; VC Dimension&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Intuition&lt;/strong&gt; even with infinitely many hypotheses, there may be only finitely many effective hypotheses.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Definitions&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Growth function/shattering number $S_n(\mathcal{F})=\sup_{\boldsymbol{x}_1,\dots,\boldsymbol{x}_n}|{(f(\boldsymbol{x}_1),\dots,f(\boldsymbol{x}_n)):f\in\mathcal{F}}|$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;This is an integer between $1$ and $2^n$.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;VC dimension $d_{VC}=d_{VC}(\mathcal{F})$ is the largest $k$ such that $S_k(\mathcal{F})=2^k$. If $S_k(\mathcal{F})=2^k$ for all $k$, then we define $d_{VC}=\infty$.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A set of such $k$ points is said to be shattered by $\mathcal{F}$.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;Sauer&amp;rsquo;s lemma&lt;/em&gt;: $S_n(\mathcal{F})\leq\sum_{i=1}^{d_{VC}}\begin{pmatrix}n\i\end{pmatrix}$. For $n\leq d_{VC},S_n(\mathcal{F})=2^n$. Otherwise, $S_n(\mathcal{F})\leq(\frac{d_{VC}}{n})^{d_{VC}}$. A slightly weaker bound is $S_n(\mathcal{F})\leq(n+1)^{d_{VC}}$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt; if $d_{VC}&amp;lt;\infty$, then $\mathcal{F}$ is PAC-learnable under the 0-1 loss with sample complexity&lt;/p&gt;
&lt;p&gt;$$
\bar{n}(\epsilon,\delta)=C\cdot\frac{d_{VC}+\log\frac{1}{\epsilon}}{\epsilon^2}
$$&lt;/p&gt;
&lt;p&gt;for some constant $C$. Conversely, if $d_{VC}=\infty$, then $\mathcal{F}$ is not PAC-leranable.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Hence, $d_{VC}$ serves as a fundamental measure of richness of the function class – to get good generalization, it suffices to have $n\gg d_{VC}$ samples.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Even if $d_{VC}$ is infinite, efficient learning might be possible for a given data distribution $P_{XY}$. The VC theory only establishes the difficulty of worst-case distributions.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Examples&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;rectangular classifier: $d_{VC}=4$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;linear classifier w/ and w/o an offset in dimnension $d$: $d_{VC}=d$ or $d+1$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>Archives</title>
        <link>https://wwwCielwww.github.io/archives/</link>
        <pubDate>Tue, 28 May 2019 00:00:00 +0000</pubDate>
        
        <guid>https://wwwCielwww.github.io/archives/</guid>
        <description></description>
        </item>
        <item>
        <title>About</title>
        <link>https://wwwCielwww.github.io/about/</link>
        <pubDate>Thu, 28 Feb 2019 00:00:00 +0000</pubDate>
        
        <guid>https://wwwCielwww.github.io/about/</guid>
        <description>&lt;p&gt;Written in Go, Hugo is an open source static site generator available under the &lt;a class=&#34;link&#34; href=&#34;https://github.com/gohugoio/hugo/blob/master/LICENSE&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Apache Licence 2.0.&lt;/a&gt; Hugo supports TOML, YAML and JSON data file types, Markdown and HTML content files and uses shortcodes to add rich content. Other notable features are taxonomies, multilingual mode, image processing, custom output formats, HTML/CSS/JS minification and support for Sass SCSS workflows.&lt;/p&gt;
&lt;p&gt;Hugo makes use of a variety of open source projects including:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/yuin/goldmark&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/yuin/goldmark&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/alecthomas/chroma&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/alecthomas/chroma&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/muesli/smartcrop&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/muesli/smartcrop&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/spf13/cobra&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/spf13/cobra&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/spf13/viper&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/spf13/viper&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Hugo is ideal for blogs, corporate websites, creative portfolios, online magazines, single page applications or even a website with thousands of pages.&lt;/p&gt;
&lt;p&gt;Hugo is for people who want to hand code their own website without worrying about setting up complicated runtimes, dependencies and databases.&lt;/p&gt;
&lt;p&gt;Websites built with Hugo are extremelly fast, secure and can be deployed anywhere including, AWS, GitHub Pages, Heroku, Netlify and any other hosting provider.&lt;/p&gt;
&lt;p&gt;Learn more and contribute on &lt;a class=&#34;link&#34; href=&#34;https://github.com/gohugoio&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;GitHub&lt;/a&gt;.&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Links</title>
        <link>https://wwwCielwww.github.io/links/</link>
        <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
        
        <guid>https://wwwCielwww.github.io/links/</guid>
        <description>&lt;p&gt;To use this feature, add &lt;code&gt;links&lt;/code&gt; section to frontmatter.&lt;/p&gt;
&lt;p&gt;This page&amp;rsquo;s frontmatter:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;9
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nt&#34;&gt;links&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;title&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;GitHub&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;description&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;GitHub is the world&amp;#39;s largest software development platform.&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;website&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;https://github.com&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;image&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;title&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;TypeScript&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;description&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;TypeScript is a typed superset of JavaScript that compiles to plain JavaScript.&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;website&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;https://www.typescriptlang.org&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;image&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;ts-logo-128.jpg&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;&lt;code&gt;image&lt;/code&gt; field accepts both local and external images.&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Search</title>
        <link>https://wwwCielwww.github.io/search/</link>
        <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
        
        <guid>https://wwwCielwww.github.io/search/</guid>
        <description></description>
        </item>
        
    </channel>
</rss>
